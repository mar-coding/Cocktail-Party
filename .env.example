# =============================================================================
# Production Mode
# =============================================================================
# Set to "true" for production deployment, "false" for development
# When IS_PROD=true:
#   - Authentication is required
#   - SSL verification is enforced
#   - Logging level is INFO
#   - Debug prints are disabled
IS_PROD=false

# =============================================================================
# Authentication (required when IS_PROD=true)
# =============================================================================
# Basic authentication for Gradio UI
GRADIO_USERNAME=admin
GRADIO_PASSWORD=

# =============================================================================
# Hugging Face Configuration
# =============================================================================
# Hugging Face token (optional, for gated models)
# IMPORTANT: Never commit real tokens! Generate at https://huggingface.co/settings/tokens
HF_TOKEN=

# =============================================================================
# LLM Provider Configuration
# =============================================================================
# Provider: "ollama" (default) or "vllm"
LLM_PROVIDER=ollama

# -----------------------------------------------------------------------------
# Ollama Settings (when LLM_PROVIDER=ollama)
# -----------------------------------------------------------------------------
# For Docker container: http://ollama:11434
# For native Mac (Ollama running locally): http://host.docker.internal:11434
# For Lambda Labs GPU VPS: http://YOUR_VPS_IP:11434
OLLAMA_HOST=http://ollama:11434
OLLAMA_MODEL=gemma3:4b

# -----------------------------------------------------------------------------
# vLLM Settings (when LLM_PROVIDER=vllm)
# -----------------------------------------------------------------------------
# OpenAI-compatible vLLM endpoint (e.g., RunPod serverless)
# API base URL: https://api.runpod.ai/v2/<ENDPOINT_ID>/openai/v1
VLLM_API_TOKEN=
VLLM_API_ADDRESS=
VLLM_MODEL=google/gemma-3-4b-it

# =============================================================================
# Gradio Server Settings
# =============================================================================
GRADIO_SERVER_NAME=0.0.0.0
GRADIO_SERVER_PORT=7860

# Docker mode flag (automatically set in container)
DOCKER_CONTAINER=true

# Set to "true" when running behind a reverse proxy (e.g., Nginx Proxy Manager)
# When true, SSL is handled by the proxy and not by Gradio directly
BEHIND_PROXY=false

# =============================================================================
# WebRTC Connection Settings
# =============================================================================
# Maximum number of concurrent WebRTC connections allowed
# Default: 6 (set higher for larger deployments)
MAX_CONCURRENT_CONNECTIONS=6

# =============================================================================
# SSL Configuration (required for microphone access over network)
# =============================================================================
# Browsers require HTTPS to access microphone on non-localhost origins.
# Generate certificates with: ./generate-certs.sh
#
# For self-signed certificates (development/testing):
#   GRADIO_SSL_CERTFILE=/app/certs/cert.pem
#   GRADIO_SSL_KEYFILE=/app/certs/key.pem
#
# For production with Let's Encrypt:
#   GRADIO_SSL_CERTFILE=/path/to/fullchain.pem
#   GRADIO_SSL_KEYFILE=/path/to/privkey.pem
#
# Note: These are pre-configured in docker-compose.yml

# =============================================================================
# HTTP Request Settings
# =============================================================================
# Connect timeout: how long to wait for initial connection (seconds)
HTTP_CONNECT_TIMEOUT=10
# Read timeout: how long to wait for LLM response (seconds)
# Set higher for slower models or cold starts
HTTP_READ_TIMEOUT=120
