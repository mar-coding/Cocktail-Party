name: Deploy to Production

on:
  push:
    branches:
      - main
  workflow_dispatch:  # Allow manual trigger

env:
  APP_DIR: local-voice-ai-agent
  DEPLOY_PATH: /opt/cocktail-party

jobs:
  # ==========================================================================
  # Build & Test
  # ==========================================================================
  build:
    name: Build & Validate
    runs-on: ubuntu-latest

    steps:
      - name: Checkout code
        uses: actions/checkout@v4

      - name: Set up Python
        uses: actions/setup-python@v5
        with:
          python-version: '3.13'

      - name: Validate Python syntax
        run: |
          cd ${{ env.APP_DIR }}
          python -m py_compile *.py

      - name: Validate Docker Compose
        run: |
          cd ${{ env.APP_DIR }}
          docker compose config --quiet

  # ==========================================================================
  # Deploy to Production
  # ==========================================================================
  deploy:
    name: Deploy to VPS
    runs-on: ubuntu-latest
    needs: build
    if: github.ref == 'refs/heads/main'

    steps:
      - name: Checkout code
        uses: actions/checkout@v4

      - name: Setup SSH key
        run: |
          mkdir -p ~/.ssh
          echo "${{ secrets.VPS_SSH_KEY }}" > ~/.ssh/id_rsa
          chmod 600 ~/.ssh/id_rsa
          ssh-keyscan -H ${{ secrets.VPS_HOST }} >> ~/.ssh/known_hosts

      - name: Create .env file
        run: |
          cd ${{ env.APP_DIR }}
          cat > .env << EOF
          IS_PROD=true
          GRADIO_USERNAME=${{ secrets.GRADIO_USERNAME }}
          GRADIO_PASSWORD=${{ secrets.GRADIO_PASSWORD }}
          HF_TOKEN=${{ secrets.HF_TOKEN }}
          LLM_PROVIDER=${{ secrets.LLM_PROVIDER }}
          ANTHROPIC_API_KEY=${{ secrets.ANTHROPIC_API_KEY }}
          ANTHROPIC_MODEL=claude-3-haiku-20240307
          OLLAMA_HOST=http://ollama:11434
          OLLAMA_MODEL=gemma3:4b
          GRADIO_SERVER_NAME=0.0.0.0
          GRADIO_SERVER_PORT=7860
          DOCKER_CONTAINER=true
          BEHIND_PROXY=true
          HTTP_CONNECT_TIMEOUT=10
          HTTP_READ_TIMEOUT=120
          MAX_CONCURRENT_CONNECTIONS=6
          EOF
          # Remove leading whitespace from .env
          sed -i 's/^[[:space:]]*//' .env

      - name: Deploy to VPS
        run: |
          rsync -avz --delete \
            --exclude='.git' \
            --exclude='__pycache__' \
            --exclude='*.pyc' \
            --exclude='.venv' \
            --exclude='.mypy_cache' \
            --exclude='certs/*.pem' \
            ${{ env.APP_DIR }}/ \
            ${{ secrets.VPS_USER }}@${{ secrets.VPS_HOST }}:${{ env.DEPLOY_PATH }}/

      - name: Build and restart containers
        run: |
          ssh ${{ secrets.VPS_USER }}@${{ secrets.VPS_HOST }} << 'ENDSSH'
            cd /opt/cocktail-party

            # Read LLM_PROVIDER from .env
            LLM_PROVIDER=$(grep -E "^LLM_PROVIDER=" .env | tail -1 | cut -d= -f2)
            echo "LLM Provider: $LLM_PROVIDER"

            # Build new images
            docker compose build --pull

            if [ "$LLM_PROVIDER" = "ollama" ]; then
              echo "Starting with Docker Ollama..."

              # Get model name from .env
              OLLAMA_MODEL=$(grep -E "^OLLAMA_MODEL=" .env | tail -1 | cut -d= -f2)
              OLLAMA_MODEL=${OLLAMA_MODEL:-gemma3:4b}
              echo "Ollama Model: $OLLAMA_MODEL"

              # Start Ollama first
              docker compose --profile ollama up -d ollama

              # Wait for Ollama to be healthy
              echo "Waiting for Ollama to be ready..."
              sleep 15

              # Pull model if not exists
              if ! docker exec ollama ollama list | grep -q "$OLLAMA_MODEL"; then
                echo "Pulling $OLLAMA_MODEL model (this may take a few minutes)..."
                docker exec ollama ollama pull "$OLLAMA_MODEL"
              fi

              # Start voice-app with ollama profile
              docker compose --profile ollama up -d voice-app
            else
              echo "Starting with Anthropic Claude API..."

              # Stop ollama if running
              docker compose --profile ollama down ollama 2>/dev/null || true

              # Start voice-app only (no Ollama needed)
              docker compose up -d voice-app
            fi

            # Cleanup old images
            docker image prune -f

            # Show status
            docker compose ps
          ENDSSH

      - name: Health check
        run: |
          echo "Waiting for services to start..."
          sleep 60
          ssh ${{ secrets.VPS_USER }}@${{ secrets.VPS_HOST }} << 'EOF'
            cd /opt/cocktail-party

            # Check if voice-app container is running
            if docker compose ps voice-app | grep -q "Up"; then
              echo "✅ Deployment successful!"
              docker compose ps

              # Test LLM connectivity
              LLM_PROVIDER=$(grep -E "^LLM_PROVIDER=" .env | tail -1 | cut -d= -f2)
              if [ "$LLM_PROVIDER" = "anthropic" ]; then
                echo "Testing Anthropic API..."
                docker exec voice-app python -c "import anthropic; print('Anthropic SDK OK')" && echo "✅ Anthropic ready"
              else
                echo "Testing Ollama..."
                docker exec ollama ollama list && echo "✅ Ollama ready"
              fi
            else
              echo "❌ Deployment failed!"
              docker compose logs --tail=50
              exit 1
            fi
          EOF
